
# ðŸŽµ NSynth Instrument Classification with MFCC + CNN

A PyTorch-based deep learning project that classifies musical instruments from raw audio using MFCC features and Convolutional Neural Networks (CNNs). Built and tested in Debian 12 using Anaconda virtual environments.

---

## ðŸ“ Dataset: [NSynth](https://magenta.tensorflow.org/datasets/nsynth)

> Neural Synthesizer Dataset by Google Magenta  
> 300k+ musical notes from over 1,000 instruments  
> Each note is a 4-second, 16kHz WAV file

### âœ… Folder Structure After Extraction:

```
~/datasets/nsynth/
â”œâ”€â”€ nsynth-train/
â”‚   â”œâ”€â”€ audio/            # WAV files (~60k)
â”‚   â””â”€â”€ examples.json     # metadata
â”œâ”€â”€ nsynth-valid/
â”‚   â”œâ”€â”€ audio/
â”‚   â””â”€â”€ examples.json
â””â”€â”€ nsynth-test/
    â”œâ”€â”€ audio/
    â””â”€â”€ examples.json
```

---

## ðŸ’» Environment Setup (Debian 12 + Anaconda)

```bash
# 1. Create and activate environment
conda create -n nsynth python=3.10 -y
conda activate nsynth

# 2. Install PyTorch and torchaudio (CPU or CUDA)
conda install pytorch torchaudio -c pytorch

# 3. Optional: Other utilities
pip install matplotlib tqdm
```

---

## ðŸ§  Model Overview

We use **MFCCs** (Mel-Frequency Cepstral Coefficients) as input features and feed them into a **3-layer CNN**, followed by fully connected layers for final classification into 11 instrument families.

```
WAV (16kHz) â†’ MFCC (40xTime) â†’ CNN â†’ FC â†’ Softmax (11 classes)
```

---

## ðŸ—‚ Project Structure

```
nsynth_project_mfcc/
â”œâ”€â”€ dataset.py         # Load WAVs, compute MFCCs
â”œâ”€â”€ model.py           # CNN model definition
â”œâ”€â”€ train.py           # Training loop
â”œâ”€â”€ eval.py            # Final test set evaluation
â”œâ”€â”€ mfcc_audio_model.pth   # Saved model (after training)
â””â”€â”€ README.md
```

---

## ðŸ§¾ dataset.py

Loads NSynth audio and computes MFCCs (using `torchaudio.transforms.MFCC`).  
Returns `(1, 40, T)` MFCC tensors + integer labels (`0~10`).

---

## ðŸ§± model.py

A lightweight CNN model for MFCC inputs:

- Conv2d(1 â†’ 16 â†’ 32 â†’ 64)
- ReLU + MaxPooling
- Flatten â†’ FC â†’ Output (11)

```python
x â†’ [B, 1, 40, Time] â†’ CNN â†’ [B, 64, H, W] â†’ FC â†’ [B, 11]
```

---

## ðŸš€ train.py

Trains the model with CrossEntropyLoss + Adam optimizer for N epochs.

```bash
python train.py
```

It will print loss and validation accuracy for each epoch and save the model to:

```bash
mfcc_audio_model.pth
```

---

## ðŸ§ª eval.py

Loads the saved model and evaluates performance on the **test set**:

```bash
python eval.py
```

Example output:

```
Test Accuracy: 72.89%
```

---

## ðŸ“Š Results

| Input Features | Accuracy (valid) | Accuracy (test) | Epochs |
|----------------|------------------|------------------|--------|
| Raw MFCC (40xTime) | ~68â€“72%        | ~68â€“73%          | 10     |

---

## ðŸ“Œ Future Improvements

- Use log-mel spectrogram or delta-MFCC
- Try deeper CNN / ResNet / Transformer
- Fine-tune on custom instruments
- Add training plots & confusion matrix

---

## ðŸ“š Reference

- [NSynth Dataset](https://magenta.tensorflow.org/datasets/nsynth)
- [Torchaudio MFCC Docs](https://pytorch.org/audio/stable/transforms.html#mfcc)
- [PyTorch Official Docs](https://pytorch.org)

---

## ðŸ”’ License

MIT License
